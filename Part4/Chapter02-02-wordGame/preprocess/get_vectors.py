from dotenv import load_dotenv
import os
import pickle
import unicodedata
import re
from typing import Set
import time
from openai import OpenAI

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™”
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# í•œê¸€ ë‹¨ì–´ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def is_hangul(text) -> bool:
    return bool(re.match(r'^[\u3130-\u318F\uAC00-\uD7A3]+$', text))

# ë‹¨ì–´ ë¡œë“œ
def load_dic(path: str) -> Set[str]:
    rtn = set()
    with open(path, 'r', encoding='utf-8') as f:
        for line in f.readlines():
            word = line.strip()
            word = unicodedata.normalize('NFC', word)
            if is_hangul(word):
                rtn.add(word)
    return list(rtn)

# OpenAI APIë¥¼ í†µí•œ ë²¡í„° ìƒì„± í•¨ìˆ˜
def get_embedding(word, model="text-embedding-3-small"):
    response = client.embeddings.create(
        input=word,
        model=model,
        dimensions = 10
    )
    return response.data 

# ë‹¨ì–´ ë²¡í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
def create_word_vector_dict(word_list):
    word_vecs = {}
    emb_vectors = get_embedding(word_list)
    for idx,vec in enumerate(emb_vectors):
        if vec is not None:
            word_vecs[word_list[idx]] = vec.embedding
    return word_vecs

# ë²¡í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
def save_word_vectors(word_vecs, save_path):
    with open(save_path, 'wb') as file:
        pickle.dump(word_vecs, file)
    print(f"âœ… ë‹¨ì–´ ë²¡í„°ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
wordlist_path = '../data/words_dataset.txt'
save_path = '../data/words_vectors.pkl'

# ì‹¤í–‰
print("ğŸ“¥ ë‹¨ì–´ ì‚¬ì „ ë¡œë”© ì¤‘...")
normal_words = load_dic(wordlist_path)
print(f"ì´ {len(normal_words)}ê°œì˜ í•œê¸€ ë‹¨ì–´ ë¡œë“œ ì™„ë£Œ.")

print("ğŸ” ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘ (OpenAI API ì‚¬ìš©)...")
word_vecs = create_word_vector_dict(normal_words)

print("ğŸ’¾ ë²¡í„° ì €ì¥ ì¤‘...")
save_word_vectors(word_vecs, save_path)

print("ì™„ë£Œ")
# print(word_vecs)
